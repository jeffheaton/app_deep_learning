{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRuM9Pg2oBZQ"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jeffheaton/app_deep_learning/blob/main/t81_558_class_06_5_prompt_engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPIDsF57oBZR"
   },
   "source": [
    "# T81-558: Applications of Deep Neural Networks\n",
    "**Module 11: Natural Language Processing and Speech Recognition**\n",
    "* Instructor: [Jeff Heaton](https://sites.wustl.edu/jeffheaton/), McKelvey School of Engineering, [Washington University in St. Louis](https://engineering.wustl.edu/Programs/Pages/default.aspx)\n",
    "* For more information visit the [class website](https://sites.wustl.edu/jeffheaton/t81-558/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iEXcrLBoBZS"
   },
   "source": [
    "# Module 6 Material\n",
    "\n",
    "* Part 6.1: Introduction to Transformers [[Video]](https://www.youtube.com/watch?v=mn6r5PYJcu0&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_1_transformers.ipynb)\n",
    "* Part 6.2: Accessing the ChatGPT API [[Video]](https://www.youtube.com/watch?v=tcdscXl4o5w&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_2_chat_gpt.ipynb)\n",
    "* Part 6.3: LLM Memory [[Video]](https://www.youtube.com/watch?v=oGQ3TQx1Qs8&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_3_llm_memory.ipynb)\n",
    "* Part 6.4: Introduction to Embeddings [[Video]](https://www.youtube.com/watch?v=e6kcs9Uj_ps&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_4_embedding.ipynb)\n",
    "* **Part 6.5: Prompt Engineering** [[Video]](https://www.youtube.com/watch?v=miTpIDR7k6c&list=PLjy4p-07OYzulelvJ5KVaT2pDlxivl_BN) [[Notebook]](t81_558_class_06_5_prompt_engineering.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92qJtJOToBZS"
   },
   "source": [
    "# Google CoLab Instructions\n",
    "\n",
    "The following code ensures that Google CoLab is running the correct version of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvTVJyBMoBZS",
    "outputId": "13a4c525-a091-4ede-98dc-b814c34e0f44"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import drive\n",
    "    COLAB = True\n",
    "    print(\"Note: using Google CoLab\")\n",
    "except:\n",
    "    print(\"Note: not using Google CoLab\")\n",
    "    COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTUM5nqjoBZT"
   },
   "source": [
    "# Part 6.5: Prompt Engineering\n",
    "\n",
    "Prompt engineering is an art and science of designing effective prompts to elicit desired responses from Large Language Models (LLMs). In the era of GPT-3, GPT-4, and subsequent models, crafting the right prompt can significantly impact the accuracy, relevance, and nuance of an LLM's response. This section delves into the intricacies of prompt engineering, its importance, and strategies for achieving desired outcomes.\n",
    "\n",
    "### Why Prompt Engineering Matters\n",
    "\n",
    "In the rapidly evolving landscape of machine learning and natural language processing, Large Language Models (LLMs) have emerged as a vanguard of computational prowess. While their sheer capacity to generate and understand text is undeniably impressive, the success of their applications heavily depends on the quality of interactions they receive. Enter the crucial role of prompt engineering. This discipline serves as the bridge between the user's intent and the model's vast knowledge, determining the efficiency, relevance, and even ethical considerations of the output. Just as a seasoned interviewer knows the right questions to extract deep insights, prompt engineering shapes the way we query LLMs, ensuring we derive maximum value from these sophisticated tools. Now, let's delve into the specifics of why this matters.\n",
    "\n",
    "* Quality Control: The way a prompt is phrased can determine whether the model’s response will be on-point or entirely off the mark. A well-engineered prompt can lead to more accurate and informative answers.\n",
    "\n",
    "* Cost Efficiency: With models that charge by token (words/characters), concise and precise prompts can save computation time and costs.\n",
    "\n",
    "* Bias Mitigation: Thoughtful prompt design can help in obtaining unbiased and objective responses, especially in topics prone to subjectivity.\n",
    "\n",
    "### Principles of Effective Prompt Engineering\n",
    "\n",
    "The art of crafting questions to elicit insightful answers is as old as human dialogue itself. But when directing questions at machine systems, especially the sophisticated realm of Large Language Models, the nuances are quite distinct. The principles of effective prompt engineering provide a roadmap, guiding users to shape their inquiries in ways that harness the full potential of LLMs. It's not merely about asking a question; it's about framing, clarity, and precision. These principles reflect an understanding of the model's architecture, biases, and response mechanisms, ensuring that we're not just communicating, but communicating effectively. As we delve into the core tenets of prompt engineering, we'll uncover the subtle art of eliciting the most accurate and comprehensive answers from these digital behemoths.\n",
    "\n",
    "* Clarity: A prompt should be clearly worded. Ambiguous prompts can yield unexpected or irrelevant answers.\n",
    "\n",
    "* Specificity: It's often useful to be specific about the kind of answer or format you want. For example, instead of asking, \"Tell me about whales,\" you could ask, \"Provide a concise overview of the life cycle of blue whales.\"\n",
    "\n",
    "* Iteration: Prompt engineering is an iterative process. If a response is unsatisfactory, adjust the prompt and try again. This iterative refinement often leads to better prompts over time.\n",
    "\n",
    "### Strategies and Techniques\n",
    "\n",
    "The realm of engaging with Large Language Models is akin to navigating a vast ocean with pockets of hidden treasures. While understanding the general contours of this ocean is valuable, it is the strategies and techniques of prompt engineering that act as the navigator's compass and map. These methodologies provide practical tools for traversing the complexities of LLM interactions, ensuring that users not only reach their desired destinations but also discover richer, more nuanced insights along the journey. As we explore these strategies and techniques, we'll learn how to tailor our approach, refining our prompts to extract clarity, depth, and precision from the vast knowledge reservoirs of LLMs.\n",
    "\n",
    "* Prompt Pivoting: If an initial approach doesn’t provide the desired answer, consider asking the question in a different way or from a different angle.\n",
    "\n",
    "* Explicit Instructions: Specify the format or structure of the answer you want. For example: \"List the following in bullet points...\" or \"Provide an answer in no more than three sentences.\"\n",
    "\n",
    "* Grounding Context: Give the model context if necessary. Instead of just asking, \"How does it work?\", you might say, \"Explain how photosynthesis works in plants.\"\n",
    "\n",
    "* Challenge Assumptions: LLMs might make assumptions based on the most common interpretation of a prompt. If you want to bypass these assumptions, state them explicitly. For example, \"Ignoring financial constraints, explain the process of space tourism.\"\n",
    "\n",
    "* Bias Checks: If you're concerned about potential biases in the model's answer, you can request the model to provide multiple viewpoints or explicitly ask for an unbiased response.\n",
    "\n",
    "* Temperature and Max Tokens: Some models allow for parameters like \"temperature\" (a measure of randomness) and \"max tokens\" (limit on response length). Playing with these parameters can affect the model's verbosity and creativity.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "Venturing into the world of Large Language Models brings with it a promise of transformative insights, but like any expedition into uncharted territories, it's not without its challenges. Even as we harness sophisticated strategies and adhere to established principles, the dynamic nature of LLMs can present hurdles that require both foresight and adaptability. These challenges remind us that while technology has made leaps and bounds, it's not infallible. Recognizing and navigating these potential pitfalls is integral to ensuring a productive interaction with LLMs. As we delve deeper into these challenges, we will equip ourselves with a more holistic understanding, preparing to meet the unexpected and ensuring that our engagements with these digital giants are both meaningful and informed.\n",
    "\n",
    "* Overfitting to Prompts: Relying too heavily on a specific set of prompts for all situations can lead to overfitting, where the model might provide stereotyped or overly narrow responses.\n",
    "\n",
    "* Unexpected Responses: No matter how well you craft your prompt, LLMs might sometimes produce unexpected answers. It's essential to verify and fact-check critical information.\n",
    "\n",
    "* Bias and Ethical Concerns: LLMs can reflect biases present in their training data. Even with perfect prompt engineering, users should approach answers with a critical mindset, especially on sensitive topics.\n",
    "\n",
    "Prompt engineering is a powerful tool in harnessing the full potential of LLMs. As these models continue to evolve and become even more complex, the nuances of interacting with them effectively will only grow in importance. By mastering the principles and strategies of prompt engineering, users can achieve more accurate, efficient, and insightful interactions with LLMs.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "t81_558_class_11_05_embedding.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
